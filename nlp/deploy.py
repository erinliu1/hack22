# -*- coding: utf-8 -*-
"""hack22.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ASaKAccJRPHtBn7HJChDhFYwjGKOMBbw
"""

import numpy as np 
import pandas as pd 
import os

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

df = pd.read_csv("training_set_rel3.tsv", sep='\t', encoding='ISO-8859-1')
df = df.dropna(axis=1)
df = df.drop(columns=['rater1_domain1', 'rater2_domain1', 'essay_id', 'essay_set'])
df['domain1_score'] = df['domain1_score'].apply(lambda x: 10 * x)
df.head()

def prepare_sequence(seq, to_ix):
    idxs = [to_ix[w] for w in seq]
    return torch.tensor(idxs, dtype=torch.long)

df = df.reset_index()  # make sure indexes pair with number of rows

loaded_data = [(row['essay'].split(), row['domain1_score']) for index, row in df.iterrows()]
training_data = loaded_data[:12000]
testing_data = loaded_data[12000:]

word_to_ix = {}

for sent, tags in loaded_data:
    for word in sent:
        if word not in word_to_ix:  # word has not been assigned an index yet
            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index

VOCAB_SIZE = len(word_to_ix)

class BOWClassifier(nn.Module):
    def __init__(self, vocab_size):
        super(BOWClassifier, self).__init__()
        self.lin1 = nn.Linear(vocab_size, 1000)
        self.relu = nn.ReLU()
        self.lin2 = nn.Linear(1000, 1)
    def forward(self, x):
        x = self.lin1(x)
        x = self.relu(x)
        x = self.lin2(x)
        return x

def make_bow_vector(sentence, word_to_ix):
    vec = torch.zeros(len(word_to_ix))
    
    for word in sentence:
        if word not in word_to_ix:
            raise ValueError('houston we have a problem')
        else:
            vec[word_to_ix[word]]+=1
    return vec.view(1, -1)

model = BOWClassifier(VOCAB_SIZE).to(device)
# loss_fn = nn.MSELoss().to(device)
# optimizer = optim.Adam(model.parameters(), lr=0.01)
# 
# # the training loop
# for epoch in range(10):
#     print(f'--- Epoch {epoch} ---')
#     epochloss = 0
#     count = 0
#     for instance, label in training_data:
#         # get the training data
#         count += 1
#         model.zero_grad()
#         bow_vec = Variable(make_bow_vector(instance, word_to_ix)).to(device)
#         probs = model(bow_vec) # forward pass
#         loss = loss_fn(probs[0], torch.tensor(label).float()[None].to(device))
#         loss.backward()
#         optimizer.step()
#         
#         epochloss += loss.data
#         if count % 100 == 0:
#             print('AVERAGE LOSS: {}'.format(loss.data / len(training_data)))
# 
#     print('TOTAL EPOCH LOSS: {}'.format(epochloss))
# 
# print('--- AFTER TRAINING ---')
# for instance, label in testing_data:
#     bow_vec = Variable(make_bow_vector(instance, word_to_ix)).to(device)
#     prediction = model(bow_vec)
#     print(f'prediction: {prediction}')
#     print(f'actual: {label}')
# 
# torch.save(model.state_dict(), "model_weights.pth") # save my model
# print("Saved PyTorch Model State to model_weights.pth")

# model = resnet18(weights=None).to(device) # load my model
#model.load_state_dict(torch.load("model.pth", map_location=torch.device('cpu'))) # depends if running on cpu/gpu
model.load_state_dict(torch.load("model_weights.pth"))

def predict_score(sentence):
  for i in range(len(sentence)):
    if sentence[i] not in word_to_ix:
      sentence[i] = 'the'
  sentence_bow = Variable(make_bow_vector(sentence, word_to_ix)).to(device)
  return round(model(sentence_bow).item())

print(predict_score(test_sentence.split()))